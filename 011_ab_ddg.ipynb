{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import torch.linalg\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "from Bio import BiopythonWarning\n",
    "from Bio.PDB import Selection\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.PDB.Polypeptide import three_to_one, three_to_index, is_aa\n",
    "\n",
    "import math\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "from models.geo_ddg.predictor import DDGPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Github](https://github.com/HeliXonProtein/binding-ddg-predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackHole(object):\n",
    "    def __setattr__(self, name, value):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return self\n",
    "\n",
    "\n",
    "def seed_all(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def recursive_to(obj, device):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        try:\n",
    "            return obj.cuda(device=device, non_blocking=True)\n",
    "        except RuntimeError:\n",
    "            return obj.to(device)\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_to(o, device=device) for o in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return (recursive_to(o, device=device) for o in obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: recursive_to(v, device=device) for k, v in obj.items()}\n",
    "\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NON_STANDARD_SUBSTITUTIONS = {\n",
    "    '2AS':'ASP', '3AH':'HIS', '5HP':'GLU', 'ACL':'ARG', 'AGM':'ARG', 'AIB':'ALA', 'ALM':'ALA', 'ALO':'THR', 'ALY':'LYS', 'ARM':'ARG',\n",
    "    'ASA':'ASP', 'ASB':'ASP', 'ASK':'ASP', 'ASL':'ASP', 'ASQ':'ASP', 'AYA':'ALA', 'BCS':'CYS', 'BHD':'ASP', 'BMT':'THR', 'BNN':'ALA',\n",
    "    'BUC':'CYS', 'BUG':'LEU', 'C5C':'CYS', 'C6C':'CYS', 'CAS':'CYS', 'CCS':'CYS', 'CEA':'CYS', 'CGU':'GLU', 'CHG':'ALA', 'CLE':'LEU', 'CME':'CYS',\n",
    "    'CSD':'ALA', 'CSO':'CYS', 'CSP':'CYS', 'CSS':'CYS', 'CSW':'CYS', 'CSX':'CYS', 'CXM':'MET', 'CY1':'CYS', 'CY3':'CYS', 'CYG':'CYS',\n",
    "    'CYM':'CYS', 'CYQ':'CYS', 'DAH':'PHE', 'DAL':'ALA', 'DAR':'ARG', 'DAS':'ASP', 'DCY':'CYS', 'DGL':'GLU', 'DGN':'GLN', 'DHA':'ALA',\n",
    "    'DHI':'HIS', 'DIL':'ILE', 'DIV':'VAL', 'DLE':'LEU', 'DLY':'LYS', 'DNP':'ALA', 'DPN':'PHE', 'DPR':'PRO', 'DSN':'SER', 'DSP':'ASP',\n",
    "    'DTH':'THR', 'DTR':'TRP', 'DTY':'TYR', 'DVA':'VAL', 'EFC':'CYS', 'FLA':'ALA', 'FME':'MET', 'GGL':'GLU', 'GL3':'GLY', 'GLZ':'GLY',\n",
    "    'GMA':'GLU', 'GSC':'GLY', 'HAC':'ALA', 'HAR':'ARG', 'HIC':'HIS', 'HIP':'HIS', 'HMR':'ARG', 'HPQ':'PHE', 'HTR':'TRP', 'HYP':'PRO',\n",
    "    'IAS':'ASP', 'IIL':'ILE', 'IYR':'TYR', 'KCX':'LYS', 'LLP':'LYS', 'LLY':'LYS', 'LTR':'TRP', 'LYM':'LYS', 'LYZ':'LYS', 'MAA':'ALA', 'MEN':'ASN',\n",
    "    'MHS':'HIS', 'MIS':'SER', 'MLE':'LEU', 'MPQ':'GLY', 'MSA':'GLY', 'MSE':'MET', 'MVA':'VAL', 'NEM':'HIS', 'NEP':'HIS', 'NLE':'LEU',\n",
    "    'NLN':'LEU', 'NLP':'LEU', 'NMC':'GLY', 'OAS':'SER', 'OCS':'CYS', 'OMT':'MET', 'PAQ':'TYR', 'PCA':'GLU', 'PEC':'CYS', 'PHI':'PHE',\n",
    "    'PHL':'PHE', 'PR3':'CYS', 'PRR':'ALA', 'PTR':'TYR', 'PYX':'CYS', 'SAC':'SER', 'SAR':'GLY', 'SCH':'CYS', 'SCS':'CYS', 'SCY':'CYS',\n",
    "    'SEL':'SER', 'SEP':'SER', 'SET':'SER', 'SHC':'CYS', 'SHR':'LYS', 'SMC':'CYS', 'SOC':'CYS', 'STY':'TYR', 'SVA':'SER', 'TIH':'ALA',\n",
    "    'TPL':'TRP', 'TPO':'THR', 'TPQ':'ALA', 'TRG':'LYS', 'TRO':'TRP', 'TYB':'TYR', 'TYI':'TYR', 'TYQ':'TYR', 'TYS':'TYR', 'TYY':'TYR'\n",
    "}\n",
    "\n",
    "RESIDUE_SIDECHAIN_POSTFIXES = {\n",
    "    'A': ['B'],\n",
    "    'R': ['B', 'G', 'D', 'E', 'Z', 'H1', 'H2'],\n",
    "    'N': ['B', 'G', 'D1', 'D2'],\n",
    "    'D': ['B', 'G', 'D1', 'D2'],\n",
    "    'C': ['B', 'G'],\n",
    "    'E': ['B', 'G', 'D', 'E1', 'E2'],\n",
    "    'Q': ['B', 'G', 'D', 'E1', 'E2'],\n",
    "    'G': [],\n",
    "    'H': ['B', 'G', 'D1', 'D2', 'E1', 'E2'],\n",
    "    'I': ['B', 'G1', 'G2', 'D1'],\n",
    "    'L': ['B', 'G', 'D1', 'D2'],\n",
    "    'K': ['B', 'G', 'D', 'E', 'Z'],\n",
    "    'M': ['B', 'G', 'D', 'E'],\n",
    "    'F': ['B', 'G', 'D1', 'D2', 'E1', 'E2', 'Z'],\n",
    "    'P': ['B', 'G', 'D'],\n",
    "    'S': ['B', 'G'],\n",
    "    'T': ['B', 'G1', 'G2'],\n",
    "    'W': ['B', 'G', 'D1', 'D2', 'E1', 'E2', 'E3', 'Z2', 'Z3', 'H2'],\n",
    "    'Y': ['B', 'G', 'D1', 'D2', 'E1', 'E2', 'Z', 'H'],    \n",
    "    'V': ['B', 'G1', 'G2'],\n",
    "}\n",
    "\n",
    "GLY_INDEX = 5\n",
    "ATOM_N, ATOM_CA, ATOM_C, ATOM_O, ATOM_CB = 0, 1, 2, 3, 4\n",
    "\n",
    "\n",
    "\n",
    "def augmented_three_to_one(three):\n",
    "    if three in NON_STANDARD_SUBSTITUTIONS:\n",
    "        three = NON_STANDARD_SUBSTITUTIONS[three]\n",
    "    return three_to_one(three)\n",
    "\n",
    "\n",
    "def augmented_three_to_index(three):\n",
    "    if three in NON_STANDARD_SUBSTITUTIONS:\n",
    "        three = NON_STANDARD_SUBSTITUTIONS[three]\n",
    "    return three_to_index(three)\n",
    "\n",
    "\n",
    "def augmented_is_aa(three):\n",
    "    if three in NON_STANDARD_SUBSTITUTIONS:\n",
    "        three = NON_STANDARD_SUBSTITUTIONS[three]\n",
    "    return is_aa(three, standard=True)\n",
    "\n",
    "\n",
    "def is_hetero_residue(res):\n",
    "    return len(res.id[0].strip()) > 0\n",
    "\n",
    "\n",
    "def get_atom_name_postfix(atom):\n",
    "    name = atom.get_name()\n",
    "    if name in ('N', 'CA', 'C', 'O'):\n",
    "        return name\n",
    "    if name[-1].isnumeric():\n",
    "        return name[-2:]\n",
    "    else:\n",
    "        return name[-1:]\n",
    "\n",
    "\n",
    "def get_residue_pos14(res):\n",
    "    pos14 = torch.full([14, 3], float('inf'))\n",
    "    suffix_to_atom = {get_atom_name_postfix(a):a for a in res.get_atoms()}\n",
    "    atom_order = ['N', 'CA', 'C', 'O'] + RESIDUE_SIDECHAIN_POSTFIXES[augmented_three_to_one(res.get_resname())]\n",
    "    for i, atom_suffix in enumerate(atom_order):\n",
    "        if atom_suffix not in suffix_to_atom: continue\n",
    "        pos14[i,0], pos14[i,1], pos14[i,2] = suffix_to_atom[atom_suffix].get_coord().tolist()\n",
    "    return pos14\n",
    "\n",
    "\n",
    "def parse_pdb(path, model_id=0):\n",
    "    warnings.simplefilter('ignore', BiopythonWarning)\n",
    "    parser = PDBParser()\n",
    "    structure = parser.get_structure(None, path)\n",
    "    return parse_complex(structure, model_id)\n",
    "\n",
    "\n",
    "def parse_complex(structure, model_id=None):\n",
    "    if model_id is not None:\n",
    "        structure = structure[model_id]\n",
    "    chains = Selection.unfold_entities(structure, 'C')\n",
    "\n",
    "    aa, resseq, icode, seq = [], [], [], []\n",
    "    pos14, pos14_mask = [], []\n",
    "    chain_id, chain_seq = [], []\n",
    "    for i, chain in enumerate(chains):\n",
    "        seq_this = 0\n",
    "        for res in chain:\n",
    "            resname = res.get_resname()\n",
    "            if not augmented_is_aa(resname): continue\n",
    "            if not (res.has_id('CA') and res.has_id('C') and res.has_id('N')): continue\n",
    "\n",
    "            # Chain\n",
    "            chain_id.append(chain.get_id())\n",
    "            chain_seq.append(i+1)\n",
    "\n",
    "            # Residue types\n",
    "            restype = augmented_three_to_index(resname)\n",
    "            aa.append(restype)\n",
    "\n",
    "            # Atom coordinates\n",
    "            pos14_this = get_residue_pos14(res)\n",
    "            pos14_mask_this = pos14_this.isfinite()\n",
    "            pos14.append(pos14_this.nan_to_num(posinf=99999))\n",
    "            pos14_mask.append(pos14_mask_this)\n",
    "            \n",
    "            # Sequential number\n",
    "            resseq_this = int(res.get_id()[1])\n",
    "            icode_this = res.get_id()[2]\n",
    "            if seq_this == 0:\n",
    "                seq_this = 1\n",
    "            else:\n",
    "                d_resseq = resseq_this - resseq[-1]\n",
    "                if d_resseq == 0: seq_this += 1\n",
    "                else: seq_this += d_resseq\n",
    "            resseq.append(resseq_this)\n",
    "            icode.append(icode_this)\n",
    "            seq.append(seq_this)\n",
    "\n",
    "    if len(aa) == 0:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'name': structure.get_id(),\n",
    "\n",
    "        # Chain\n",
    "        'chain_id': ''.join(chain_id),\n",
    "        'chain_seq': torch.LongTensor(chain_seq),\n",
    "\n",
    "        # Sequence\n",
    "        'aa': torch.LongTensor(aa), \n",
    "        'resseq': torch.LongTensor(resseq), \n",
    "        'icode': ''.join(icode), \n",
    "        'seq': torch.LongTensor(seq), \n",
    "        \n",
    "        # Atom positions\n",
    "        'pos14': torch.stack(pos14), \n",
    "        'pos14_mask': torch.stack(pos14_mask),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingCollate(object):\n",
    "\n",
    "    def __init__(self, length_ref_key='mutation_mask', pad_values={'aa': 20, 'pos14': float('999'), 'icode': ' ', 'chain_id': '-'}, donot_pad={'foldx'}, eight=False):\n",
    "        super().__init__()\n",
    "        self.length_ref_key = length_ref_key\n",
    "        self.pad_values = pad_values\n",
    "        self.donot_pad = donot_pad\n",
    "        self.eight = eight\n",
    "\n",
    "    def _pad_last(self, x, n, value=0):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            assert x.size(0) <= n\n",
    "            if x.size(0) == n:\n",
    "                return x\n",
    "            pad_size = [n - x.size(0)] + list(x.shape[1:])\n",
    "            pad = torch.full(pad_size, fill_value=value).to(x)\n",
    "            return torch.cat([x, pad], dim=0)\n",
    "        elif isinstance(x, list):\n",
    "            pad = [value] * (n - len(x))\n",
    "            return x + pad\n",
    "        elif isinstance(x, str):\n",
    "            if value == 0:  # Won't pad strings if not specified\n",
    "                return x\n",
    "            pad = value * (n - len(x))\n",
    "            return x + pad\n",
    "        elif isinstance(x, dict):\n",
    "            padded = {}\n",
    "            for k, v in x.items():\n",
    "                if k in self.donot_pad:\n",
    "                    padded[k] = v\n",
    "                else:\n",
    "                    padded[k] = self._pad_last(v, n, value=self._get_pad_value(k))\n",
    "            return padded\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_pad_mask(l, n):\n",
    "        return torch.cat([\n",
    "            torch.ones([l], dtype=torch.bool),\n",
    "            torch.zeros([n-l], dtype=torch.bool)\n",
    "        ], dim=0)\n",
    "\n",
    "    def _get_pad_value(self, key):\n",
    "        if key not in self.pad_values:\n",
    "            return 0\n",
    "        return self.pad_values[key]\n",
    "\n",
    "    def __call__(self, data_list):\n",
    "        max_length = max([data[self.length_ref_key].size(0) for data in data_list])\n",
    "        if self.eight:\n",
    "            max_length = math.ceil(max_length / 8) * 8\n",
    "        data_list_padded = []\n",
    "        for data in data_list:\n",
    "            data_padded = {\n",
    "                k: self._pad_last(v, max_length, value=self._get_pad_value(k))\n",
    "                for k, v in data.items() if k in ('wt', 'mut', 'ddG', 'mutation_mask', 'index', 'mutation')\n",
    "            }\n",
    "            data_padded['mask'] = self._get_pad_mask(data[self.length_ref_key].size(0), max_length)\n",
    "            data_list_padded.append(data_padded)\n",
    "        return default_collate(data_list_padded)\n",
    "\n",
    "\n",
    "def _mask_list(l, mask):\n",
    "    return [l[i] for i in range(len(l)) if mask[i]]\n",
    "\n",
    "\n",
    "def _mask_string(s, mask):\n",
    "    return ''.join([s[i] for i in range(len(s)) if mask[i]])\n",
    "\n",
    "\n",
    "def _mask_dict_recursively(d, mask):\n",
    "    out = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, torch.Tensor) and v.size(0) == mask.size(0):\n",
    "            out[k] = v[mask]\n",
    "        elif isinstance(v, list) and len(v) == mask.size(0):\n",
    "            out[k] = _mask_list(v, mask)\n",
    "        elif isinstance(v, str) and len(v) == mask.size(0):\n",
    "            out[k] = _mask_string(v, mask)\n",
    "        elif isinstance(v, dict):\n",
    "            out[k] = _mask_dict_recursively(v, mask)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "class KnnResidue(object):\n",
    "\n",
    "    def __init__(self, num_neighbors=128):\n",
    "        super().__init__()\n",
    "        self.num_neighbors = num_neighbors\n",
    "\n",
    "    def __call__(self, data):\n",
    "        pos_CA = data['wt']['pos14'][:, ATOM_CA]\n",
    "        pos_CA_mut = pos_CA[data['mutation_mask']]\n",
    "        diff = pos_CA_mut.view(1, -1, 3) - pos_CA.view(-1, 1, 3)\n",
    "        dist = torch.linalg.norm(diff, dim=-1)\n",
    "\n",
    "        try:\n",
    "            mask = torch.zeros([dist.size(0)], dtype=torch.bool)\n",
    "            mask[ dist.min(dim=1)[0].argsort()[:self.num_neighbors] ] = True\n",
    "        except IndexError as e:\n",
    "            print(data)\n",
    "            raise e\n",
    "\n",
    "        return _mask_dict_recursively(data, mask)\n",
    "\n",
    "\n",
    "def load_wt_mut_pdb_pair(wt_path, mut_path):\n",
    "\n",
    "    data_wt = parse_pdb(wt_path)\n",
    "    data_mut = parse_pdb(mut_path)\n",
    "\n",
    "    transform = KnnResidue()\n",
    "    collate_fn = PaddingCollate()\n",
    "    mutation_mask = (data_wt['aa'] != data_mut['aa'])\n",
    "    batch = collate_fn([transform({'wt': data_wt, 'mut': data_mut, 'mutation_mask': mutation_mask})])\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_pdb = './testdata/geo_ddg/example_wt.pdb'\n",
    "mut_pdb = './testdata/geo_ddg/example_mut.pdb'\n",
    "model = './testdata/geo_ddg/model.pt'\n",
    "device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = load_wt_mut_pdb_pair(wt_pdb, mut_pdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = recursive_to(batch, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'node_feat_dim': 128,\n",
       "  'pair_feat_dim': 64,\n",
       "  'max_relpos': 32,\n",
       "  'geomattn': {'num_layers': 3, 'spatial_attn_mode': 'CB'}},\n",
       " 'train': {'loss_weights': {'ddG': 1.0},\n",
       "  'max_iters': 10000000,\n",
       "  'val_freq': 1000,\n",
       "  'batch_size': 8,\n",
       "  'seed': 2021,\n",
       "  'max_grad_norm': 50.0,\n",
       "  'optimizer': {'type': 'adam',\n",
       "   'lr': 0.0001,\n",
       "   'weight_decay': 0.0,\n",
       "   'beta1': 0.9,\n",
       "   'beta2': 0.999},\n",
       "  'scheduler': {'type': 'plateau',\n",
       "   'factor': 0.5,\n",
       "   'patience': 10,\n",
       "   'min_lr': 1e-06}},\n",
       " 'datasets': {'train': {'dataset_path': './data/skempi.pt'},\n",
       "  'val': {'dataset_path': './data/skempi.pt'}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ckpt['config']\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoder.relpos_embedding.weight', 'encoder.residue_encoder.aatype_embed.weight', 'encoder.residue_encoder.torsion_embed.freq_bands', 'encoder.residue_encoder.mlp.0.weight', 'encoder.residue_encoder.mlp.0.bias', 'encoder.residue_encoder.mlp.2.weight', 'encoder.residue_encoder.mlp.2.bias', 'encoder.residue_encoder.mlp.4.weight', 'encoder.residue_encoder.mlp.4.bias', 'encoder.residue_encoder.mlp.6.weight', 'encoder.residue_encoder.mlp.6.bias', 'encoder.ga_encoder.blocks.0.spatial_coef', 'encoder.ga_encoder.blocks.0.proj_query.weight', 'encoder.ga_encoder.blocks.0.proj_key.weight', 'encoder.ga_encoder.blocks.0.proj_value.weight', 'encoder.ga_encoder.blocks.0.proj_pair_bias.weight', 'encoder.ga_encoder.blocks.0.out_transform.weight', 'encoder.ga_encoder.blocks.0.out_transform.bias', 'encoder.ga_encoder.blocks.0.layer_norm.weight', 'encoder.ga_encoder.blocks.0.layer_norm.bias', 'encoder.ga_encoder.blocks.1.spatial_coef', 'encoder.ga_encoder.blocks.1.proj_query.weight', 'encoder.ga_encoder.blocks.1.proj_key.weight', 'encoder.ga_encoder.blocks.1.proj_value.weight', 'encoder.ga_encoder.blocks.1.proj_pair_bias.weight', 'encoder.ga_encoder.blocks.1.out_transform.weight', 'encoder.ga_encoder.blocks.1.out_transform.bias', 'encoder.ga_encoder.blocks.1.layer_norm.weight', 'encoder.ga_encoder.blocks.1.layer_norm.bias', 'encoder.ga_encoder.blocks.2.spatial_coef', 'encoder.ga_encoder.blocks.2.proj_query.weight', 'encoder.ga_encoder.blocks.2.proj_key.weight', 'encoder.ga_encoder.blocks.2.proj_value.weight', 'encoder.ga_encoder.blocks.2.proj_pair_bias.weight', 'encoder.ga_encoder.blocks.2.out_transform.weight', 'encoder.ga_encoder.blocks.2.out_transform.bias', 'encoder.ga_encoder.blocks.2.layer_norm.weight', 'encoder.ga_encoder.blocks.2.layer_norm.bias', 'ddG_readout.mlp.0.weight', 'ddG_readout.mlp.0.bias', 'ddG_readout.mlp.2.weight', 'ddG_readout.mlp.2.bias', 'ddG_readout.mlp.4.weight', 'ddG_readout.mlp.4.bias', 'ddG_readout.mlp.6.weight', 'ddG_readout.mlp.6.bias', 'ddG_readout.project.weight'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = ckpt['model']\n",
    "weight.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DDGPredictor(config.model).to(device)\n",
    "model.load_state_dict(weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted ddG: -0.30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred = model(batch['wt'], batch['mut'])\n",
    "    print('Predicted ddG: %.2f' % pred.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mutation_energy_change(wt_pdb, mut_pdb, model, device):\n",
    "\n",
    "    batch = load_wt_mut_pdb_pair(wt_pdb, mut_pdb)\n",
    "    batch = recursive_to(batch, device)\n",
    "    ckpt = torch.load(model)\n",
    "    config = ckpt['config']\n",
    "    weight = ckpt['model']\n",
    "    model = DDGPredictor(config.model).to(device)\n",
    "    model.load_state_dict(weight)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred = model(batch['wt'], batch['mut'])\n",
    "        print('Predicted ddG: %.2f' % pred.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_pdb = './testdata/geo_ddg/example_wt.pdb'\n",
    "mut_pdb = './testdata/geo_ddg/example_mut.pdb'\n",
    "model = './testdata/geo_ddg/model.pt'\n",
    "device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted ddG: -0.30\n"
     ]
    }
   ],
   "source": [
    "predict_mutation_energy_change(wt_pdb, mut_pdb, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('insight')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
